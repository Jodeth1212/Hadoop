# Hadoop
Hadoop is a free and open source platform for storing and processing huge datasets ranging in size from gigabytes to petabytes. Hadoop allows clustering several computers to analyze big datasets in parallel, rather than requiring a single large computer to store and analyse the data.

![reg-page-header-Big-data-web-day 778d8f76ba9b1575a45de92f7fb09c8924e90370](https://user-images.githubusercontent.com/96971533/148649439-f96d35a8-eeeb-40fa-89de-d4bcd72cd3b1.png)

Hadoop makes it easy to make use of all of a cluster server's storage and processing capability, as well as to run distributed operations on massive volumes of data. Hadoop provides the foundation for the development of other services and applications.

Hadoop is a lifesaver when it comes to large data and analytics. Data about people, processes, items, technologies, and other things is only relevant when it reveals meaningful patterns that lead to improved decisions. Hadoop assists in overcoming the difficulty of large data's vastness:

**Resilience** — Data stored in any node is also replicated in other nodes of the cluster. This ensures fault tolerance. If one node goes down, there is always a backup of the data available in the cluster.

**Scalability** — Unlike traditional systems that have a limitation on data storage, Hadoop is scalable because it operates in a distributed environment. As the need arises, the setup can be easily expanded to include more servers that can store up to multiple petabytes of data.

**Low cost** — As Hadoop is an open-source framework, with no license to be procured, the costs are significantly lower compared to relational database systems. The use of inexpensive commodity hardware also works in its favor to keep the solution economical.

**Speed** — Hadoop's distributed file system, concurrent processing, and the MapReduce model enable running complex queries in a matter of seconds.

**Data diversity** — HDFS has the capability to store different data formats such as unstructured (e.g. videos), semi-structured (e.g. XML files), and structured. While storing data, it is not required to validate against a predefined schema. Rather, the data can be dumped in any format. Later, when retrieved, data is parsed and fitted into any schema as needed. This gives the flexibility to derive different insights using the same data.

![Hadoop-Architecture2-01](https://user-images.githubusercontent.com/96971533/148649651-0b9b7fa7-74d9-44f9-92d5-37b2291be972.jpg)

